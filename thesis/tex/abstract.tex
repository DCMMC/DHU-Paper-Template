%# -*- coding: utf-8-unix -*-
%%==================================================
%% abstract.tex for SJTU Master Thesis
%%==================================================

\begin{abstract}
光学文本识别（OCR）在计算机视觉领域作为一个有着悠长历史的研究课题，随着数字化的快速发展与普及，在文档数字化、信息取证、电子商务等领域有着重要的应用价值。
在深度学习流行的大背景下，许多该领域的学者提出了基于深度学习的方法，并取得了进步。
OCR 的许多日常应用场景都是大块文本（例如书籍、年报、发票等），这些大块文本有着明显的自然语言语义特征。
已有的 OCR 研究大多集中于图像上，鲜有探索如何结合自然语言处理对识别结果进行语义修正。
本文将带有个别识别错误的结果修正为正确的文本这一过程视作翻译的一种特例，为 OCR 识别结果的后处理修正提供了一种基于语义分析的新思路。
另一方面，随着从大规模无监督文本预训练的 BERT 语言模型的涌现，我们发现带有个别错误的 OCR 识别结果类似于 BERT 中使用的带掩码的语言模型，并且可以为我们的后处理模块提供额外的语义特征。
本文首先将介绍当前有关技术的总体现状及其优劣之处，并且总结基于深度学习的文字识别系统的整体框架。
随后，我们提出两种最初用于机器翻译的网络模型：基于字卷积的方法，以及基于 BERT 语言模型和 Transformer 的方法，并将他们成功应用到 OCR 语义修正后处理模块中。
其次，本文构建了一个基于新闻语料库的大规模合成数据集用于测试，该数据集含有 160 多万样本。
本文提出的方法在该大规模数据集上进行的实验结果相比不采用后处理模块的 OCR 识别结果，在整句精确度上提升了至少 20\%，表明了我们提出的两种方法的有效性和优越性。
并且，实验结果同时表明使用预训练 BERT 语言模型对识别结果的修正亦有帮助。

\keywords{文字识别，语义分析，深度学习， 自然语言处理}

\end{abstract}

\begin{englishabstract}
Optical text recognition (OCR) has a long history in the field of computer vision as a research topic.
With the rapid development and popularity of digitalization, it has important applications in document digitization, information forensics, e-commerce and other fields.
In the context of the popularity of deep learning, many scholars in this field have proposed and made progress in deep learning-based approaches.
Many of the everyday applications of OCR are chunks of text (e.g., books, annual reports, invoices, etc.), which have obvious semantic features of natural language.
Most of the OCR research that has been done has focused on images, and little has been done to explore how semantic corrections to recognition results can be made in conjunction with natural language processing.
This paper considers the process of correcting results with individual identification errors to correct text as a special case of translation, providing a new light of thinking based on semantic analysis for the post-processing correction of OCR identification results.
On the other hand, with the emergence of BERT language models from large-scale unsupervised text pre-training, we find that OCR recognition with individual errors results similar to the masked language models used in BERT and can provide additional semantic features for our post-processing modules.
This paper will first introduce the current state-of-the-art and its advantages and disadvantages, and summarize the overall framework of a deep learning-based word recognition system.
Subsequently, we present two network models initially used for machine translation: the word convolution-based approach, and the BERT language model and Transformer-based approach, and successfully apply them to the OCR semantic modified post-processing module.
Second, this paper constructs for testing a large-scale synthetic dataset based on a news corpus, which contains more than 1.6 million samples.
The experimental results of the proposed methods on this large dataset have improved the whole-sentence accuracy by at least 20\% compared to the OCR identification results without the post-processing module, indicating the validity and superiority of our two methods.
Moreover, the experimental results also show that the use of pre-trained BERT language models can help in the correction of recognition results.

\englishkeywords{text recognition, semantic analysis, deep learning, NLP}
\end{englishabstract}
